{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural Language Processing (NLP) is amazing! ðŸ¤–ðŸ“š It helps machines understand human language. But it's not easyâ€”negation (e.g., 'I donâ€™t like this!') makes it tricky. ðŸ˜…ðŸ’¡\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'amazing', '!', 'ðŸ¤–ðŸ“š', 'It', 'helps', 'machines', 'understand', 'human', 'language', '.', 'But', 'it', \"'s\", 'not', 'easyâ€”negation', '(', 'e.g.', ',', \"'\", 'I', 'don', 'â€™', 't', 'like', 'this', '!', \"'\", ')', 'makes', 'it', 'tricky', '.', 'ðŸ˜…ðŸ’¡']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(text)\n",
    "print(\"Word Tokenization:\", word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence Tokenization: ['Natural Language Processing (NLP) is amazing!', 'ðŸ¤–ðŸ“š It helps machines understand human language.', \"But it's not easyâ€”negation (e.g., 'I donâ€™t like this!')\", 'makes it tricky.', 'ðŸ˜…ðŸ’¡']\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens = sent_tokenize(text)\n",
    "print(\"\\nSentence Tokenization:\", sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Punctuation-based Tokenization: ['(', ')', '!', '.', \"'s\", '(', 'e.g.', ',', \"'\", '!', \"'\", ')', '.']\n"
     ]
    }
   ],
   "source": [
    "punctuation_tokens = [token for token in word_tokenize(text) if any(c in token for c in \"!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\")]\n",
    "print(\"\\nPunctuation-based Tokenization:\", punctuation_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treebank Word Tokenization: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'amazing', '!', 'ðŸ¤–ðŸ“š', 'It', 'helps', 'machines', 'understand', 'human', 'language.', 'But', 'it', \"'s\", 'not', 'easyâ€”negation', '(', 'e.g.', ',', \"'I\", 'donâ€™t', 'like', 'this', '!', \"'\", ')', 'makes', 'it', 'tricky.', 'ðŸ˜…ðŸ’¡']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "treebank_tokens = treebank_tokenizer.tokenize(text)\n",
    "print(\"\\nTreebank Word Tokenization:\", treebank_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tweet Tokenization: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'amazing', '!', 'ðŸ¤–', 'ðŸ“š', 'It', 'helps', 'machines', 'understand', 'human', 'language', '.', 'But', \"it's\", 'not', 'easy', 'â€”', 'negation', '(', 'e', '.', 'g', '.', ',', \"'\", 'I', 'don', 'â€™', 't', 'like', 'this', '!', \"'\", ')', 'makes', 'it', 'tricky', '.', 'ðŸ˜…', 'ðŸ’¡']\n"
     ]
    }
   ],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokens = tweet_tokenizer.tokenize(text)\n",
    "print(\"\\nTweet Tokenization:\", tweet_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multi-Word Expression Tokenization: ['Natural Language Processing', '(', 'NLP', ')', 'is', 'amazing', '!', 'ðŸ¤–ðŸ“š', 'It', 'helps', 'machines', 'understand', 'human', 'language', '.', 'But', 'it', \"'s\", 'not', 'easyâ€”negation', '(', 'e.g.', ',', \"'\", 'I', 'don', 'â€™', 't', 'like', 'this', '!', \"'\", ')', 'makes', 'it', 'tricky', '.', 'ðŸ˜…ðŸ’¡']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "mwe_tokenizer = MWETokenizer([('Natural', 'Language', 'Processing'), ('voice', 'assistants')], separator=' ')\n",
    "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(text))\n",
    "print(\"\\nMulti-Word Expression Tokenization:\", mwe_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TextBlob Word Tokenization: ['Natural', 'Language', 'Processing', 'NLP', 'is', 'amazing', 'ðŸ¤–ðŸ“š', 'It', 'helps', 'machines', 'understand', 'human', 'language', 'But', 'it', \"'s\", 'not', 'easyâ€”negation', 'e.g', 'I', 'don', 'â€™', 't', 'like', 'this', 'makes', 'it', 'tricky', 'ðŸ˜…ðŸ’¡']\n"
     ]
    }
   ],
   "source": [
    "blob = TextBlob(text)\n",
    "textblob_tokens = blob.words\n",
    "print(\"\\nTextBlob Word Tokenization:\", textblob_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "spaCy Tokenization: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'amazing', '!', 'ðŸ¤–', 'ðŸ“š', 'It', 'helps', 'machines', 'understand', 'human', 'language', '.', 'But', 'it', \"'s\", 'not', 'easy', 'â€”', 'negation', '(', 'e.g.', ',', \"'\", 'I', 'do', 'nâ€™t', 'like', 'this', '!', \"'\", ')', 'makes', 'it', 'tricky', '.', 'ðŸ˜…', 'ðŸ’¡']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "spacy_tokens = [token.text for token in doc]\n",
    "print(\"\\nspaCy Tokenization:\", spacy_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gensim Word Tokenization: ['natural', 'language', 'processing', 'nlp', 'is', 'amazing', 'it', 'helps', 'machines', 'understand', 'human', 'language', 'but', 'it', 'not', 'easy', 'negation', 'don', 'like', 'this', 'makes', 'it', 'tricky']\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "gensim_tokens = simple_preprocess(text)\n",
    "print(\"\\nGensim Word Tokenization:\", gensim_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Keras Tokenization: ['natural', 'language', 'processing', 'nlp', 'is', 'amazing', 'ðŸ¤–ðŸ“š', 'it', 'helps', 'machines', 'understand', 'human', 'language', 'but', \"it's\", 'not', 'easyâ€”negation', 'e', 'g', \"'i\", 'donâ€™t', 'like', 'this', \"'\", 'makes', 'it', 'tricky', 'ðŸ˜…ðŸ’¡']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "keras_tokens = text_to_word_sequence(text)\n",
    "print(\"\\nKeras Tokenization:\", keras_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
